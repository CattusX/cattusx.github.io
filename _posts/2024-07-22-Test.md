---
layout: post
comments: true
title: "PLELog Paper Note"
excerpt: "该论文于2021年ICSE年发表，主要贡献是提出了第一个应用于日志异常检测的半监督框架，在两个通用基准数据集和一个真实世界数据集上取得了良好的结果，使用的半监督框架为引入概率标签估计的伪标签方法."
date:   2024-07-2 20:40:00
mathjax: true
---
[ICSE 21']PLELog: Semi-Supervised Log-Based Anomaly Detection via Probabilistic Label Estimation


## 复现过程(简单)

- 安装环境
  - 直接在conda环境, python 3.8.3版本下安装出现错误, 具体为hdbsacn和mkl三个包的问题, 于是先安装了其他包后再安装出问题的包, 作者说mkl三个包是自动生成的, 没有依赖关系
  - 安装过程中的速度问题, 先后切换了清华源, 官方源, 速度都很慢换成中科大源后解决
    `pip config set global.index-url https://pypi.mirrors.ustc.edu.cn/simple/`
  - 结果安装还是出错, 看了[issue 16](https://github.com/LeonYang95/PLELog/issues/16)后使用`conda install -c conda-forge hdbscan`安装, 还是报错
  - 最后尝试不管版本, 直接手动安装每一个包
  - torch版本和硬件计算能力不符, 重新安装torch, ...
- 运行程序
  - `python approaches/PLELog.py`
- 结果
  - 在HDFS数据集上, 直接运行[zenodo](https://zenodo.org/records/5910349)上提供的源码, 包括数据集和语义向量, 参数不改变的情况下结果如下:
    TP: 3530, TN: 168689, FN: 193, FP: 107
    Precision = 3530 / 3637 = 97.0580, Recall = 3530 / 3723 = 94.8160 F1 score = 95.9239
- 原论文结果
  - Precision = 0.950, Recall = 0.963, F1 score = 0.957
    对比之下, 结果(F1 score)非常接近, 应该存在由依赖库不同导致的偏差, 可以接受

## 算法框架

> 术语: log message, log event, log parameters, log parsing, message sequence, log sequence, anomalous log sequence, normal log sequence

挑战

1. 如何根据已知的normal log sequence预测训练集中未知的log sequence标签 -> HDBSCAN
2. 如何减少聚类的噪声影响 -> 概率标签估计, 以及Attn-GRU
3. 如何减少日志变动带来的不稳定性 -> 语义嵌入(非原创, LogRobust)
   整体由三个部分构成

### 语义向量获取

1. FastText词向量
   词向量在代码中是直接导入`datasets/glove.6B.300d.txt`获得的, 每一行301个值, 第一个是词, 后300个是词向量, <mark style="background: #ADCCFFA6;">   这里有点冲突, 文章中说的是用300d的FastText生成词向量, 但是导入的确实glove词向量</mark>
2. Drain3日志解析
3. TF-IDF转换词向量为语义向量

### 概率标签估计

1. FastICA降维 300->reduce_dimension=50

```python
from sklearn.decomposition import FastICA
if reduce_dimension != -1:
	transformer = FastICA(n_components=reduce_dimension)
	train_reprs = transformer.fit_transform(train_reprs)
```

取出训练集改变表征后的1/2数量的正常样本, 用于概率标签估计的已知正常样本

```python
train_normal = [x for x, inst in enumerate(train) if inst.label == 'Normal']
normal_ids = train_normal[:int(0.5 * len(train_normal))]
```

2. 概率标签估计
   调用方法

```python
label_generator = Probabilistic_Labeling(min_samples=min_samples, min_clust_size=min_cluster_size,
										 res_file=prob_label_res_file, rand_state_file=rand_state)
labeled_train = label_generator.auto_label(train, normal_ids)
```

实现

```python
labels = self.model.fit_predict(inputs)
```

HDBSCAN

```python
# models/clustering.py -> Solitary_HDBSCAN
from hdbscan import HDBSCAN as dbscan

self.model = dbscan(algorithm='best',
					min_cluster_size=self.min_cluster_size,
					min_samples=self.min_samples if self.min_samples != -1 else None,
					core_dist_n_jobs=10,
					metric='euclidean')

def fit_predict(self, inputs):
	self.labels = self.model.fit_predict(inputs).tolist()
	self.clusters = set(self.labels)
	self.outliers = self.model.outlier_scores_.tolist()
	return self.labels

def predict(self, inputs, normal_ids):
	'''
	normal_ids are involved in inputs.
	:param inputs: all input reprs
	:param normal_ids: labeled normal indexes.
	:return: predicted label for each line of inputs, labeled normal ones included.
	'''
	predicted = []
	assert len(inputs) == len(self.labels)
	inputs = np.asarray(inputs, dtype=float)
	normal_matrix = [] # 记录已知正常样本的特征向量
	for id in normal_ids:
		normal_matrix.append(inputs[id, :])
		if self.labels[id] != -1:   # 噪声 / 离群点
			self.normal_cores.add(self.labels[id])
	normal_matrix = np.asarray(normal_matrix, dtype=float)

	by_normal_core_normal = 0
	by_normal_core_anomalous = 0
	by_dist_normal = 0
	by_dist_anomalous = 0

	for id, predict_cluster in enumerate(self.labels):
		if id in normal_ids:
			# Add labeled normals as predicted normals to formalize the output format for other modules.
			predicted.append('Normal')
			continue
		if predict_cluster in self.normal_cores:
			by_normal_core_normal += 1
			predicted.append('Normal')
		elif predict_cluster == -1:
			cur_repr = inputs[id]
			dists = cdist([cur_repr], normal_matrix) # (1, 50) (len(normal_ids), 50) => (1, len(normal_ids)) 表示当前样本与所有已知正常样本的距离
			if dists.min() == 0:
				by_dist_normal += 1
				predicted.append('Normal')
			else:
				by_dist_anomalous += 1
				predicted.append('Anomalous')
			pass
		else:
			by_normal_core_anomalous += 1
			predicted.append('Anomalous')
	return predicted
```

参数
	这里论文中使用的min_cluster_size和min_samples都为100
	algorithm: string, optional (default=’best’) HDBSCAN实现的算法和数据结构, 默认为自动选择最佳的
	metric: string, or callable, optional (default=’euclidean’) 点之间的距离度量方法, callable表示可自定义距离度量函数
	core_dist_n_jobs: int, optional (default=4) 核心距离计算的并行数量
	min_cluster_size 簇的最小尺寸；包含比这更少的点的单链接分裂将被视为点“脱离”簇，而不是簇分裂成两个新簇。
	min_samples 被视为核心点的点的邻域中的样本数。
labels = -1表示噪声 / 离群点
HDBSCAN标签分配: 对训练集所有的样本使用HDBSCAN进行聚类, 具体划分规则为

1. 对于训练集每一行id特征的聚类结果predict_cluster
   1. 如果id在normal_ids中, 肯定是正常样本, 标记为正常
   2. 否则
      1. if 聚类结果在normal_ids对应的所有簇中, 标记为正常
      2. else if 聚类结果是噪声(-1), 用距离判断, 计算该样本的特征与所有已知正常样本的特征的欧式距离
         如果距离最小值为0(应该就是存在相同的日志吧), 标记为正常, 否则标记为异常
      3. else 标记为异常

置信度标签分配
在HDBSCAN分配过正异常标签后, 再进行一轮遍历得到labeled_inst, 其中
若为已知正常样本, 直接增加对应的instance, 否则
	若为噪声, 则confidence=0, 即确定性最大
	若为一个簇中的样本, 则confidence定义为HDBSCAN得到的离群值(outlier)

